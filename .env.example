# Enterprise LLM Hosting Configuration

# --- Model Selection ---
# The model slug to be loaded by Ollama (e.g. llama3, mistral)
# This model will be automatically pulled on first startup.
DEFAULT_MODEL=llama3

# --- LLM Backend ---
OLLAMA_HOST=http://ollama:11434

# --- Auth Service Credentials ---
# IMPORTANT: Change these values for production!
ADMIN_USER=your-admin@example.com
ADMIN_PASSWORD=your-secure-password-here
# Token expiration in hours (1h is the default)
TOKEN_EXPIRATION_HOURS=1

# --- LLM API Access Control ---
# Comma-separated list of authorized domains
ALLOWED_DOMAINS=example.com

# Comma-separated list of authorized emails (optional override)
ALLOWED_USERS=

# Cors Configuration (optional)
# Comma-separated list of allowed origins for llm-api
# CORS_ORIGINS=https://your-frontend.com

# --- Rate Limiting (optional) ---
# To use Redis for rate limiting, uncomment the following line and
# run docker-compose with '--profile prod'.
# REDIS_URL=redis://redis:6379
